import os
import random
import requests
import numpy as np
import sounddevice as sd
import torch
import openai
import webbrowser
import scipy.io.wavfile as wavfile
from datetime import datetime
from vosk import Model, KaldiRecognizer
import json
import re
from dotenv import load_dotenv

# ===================== INIT =====================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai.api_key = OPENAI_API_KEY

SAMPLE_RATE = 16000
DEVICE = "cpu"
city = "–ò–∂–µ–≤—Å–∫"

DIALOG_HISTORY = []
MAX_HISTORY = 5

VOSK_MODEL_PATH = "vosk-model-small-ru"

SYSTEM_PROMPT = (
    "–¢—ã –î–∂–∞—Ä–≤–∏—Å. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —É–≤–µ—Ä–µ–Ω–Ω–æ, —Å –∏—Ä–æ–Ω–∏–µ–π. "
    "–í—Å–µ–≥–¥–∞ –æ–±—Ä–∞—â–∞–π—Å—è ¬´—Å—ç—Ä¬ª."
)

MUSIC_LINKS = [
    "https://www.youtube.com/watch?v=ZYAPgPH9hsI",
    "https://www.youtube.com/watch?v=BN1WwnEDWAM",
    "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
]

WEATHER_REPLACE = {
    "-": "–º–∏–Ω—É—Å ", "¬∞C": " –≥—Ä–∞–¥—É—Å–æ–≤", "C": " –≥—Ä–∞–¥—É—Å–æ–≤",
    "0": " –Ω–æ–ª—å", "1": " –æ–¥–∏–Ω", "2": " –¥–≤–∞", "3": " —Ç—Ä–∏",
    "4": " —á–µ—Ç—ã—Ä–µ", "5": " –ø—è—Ç—å", "6": " —à–µ—Å—Ç—å",
    "7": " —Å–µ–º—å", "8": " –≤–æ—Å–µ–º—å", "9": " –¥–µ–≤—è—Ç—å"
}

HOURS = {
    0: "–ø–æ–ª–Ω–æ—á—å", 1: "—á–∞—Å", 2: "–¥–≤–∞", 3: "—Ç—Ä–∏", 4: "—á–µ—Ç—ã—Ä–µ",
    5: "–ø—è—Ç—å", 6: "—à–µ—Å—Ç—å", 7: "—Å–µ–º—å", 8: "–≤–æ—Å–µ–º—å",
    9: "–¥–µ–≤—è—Ç—å", 10: "–¥–µ—Å—è—Ç—å", 11: "–æ–¥–∏–Ω–Ω–∞–¥—Ü–∞—Ç—å",
    12: "–¥–≤–µ–Ω–∞–¥—Ü–∞—Ç—å", 13: "—Ç—Ä–∏–Ω–∞–¥—Ü–∞—Ç—å", 14: "—á–µ—Ç—ã—Ä–Ω–∞–¥—Ü–∞—Ç—å",
    15: "–ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å", 16: "—à–µ—Å—Ç–Ω–∞–¥—Ü–∞—Ç—å", 17: "—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—å",
    18: "–≤–æ—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—å", 19: "–¥–µ–≤—è—Ç–Ω–∞–¥—Ü–∞—Ç—å", 20: "–¥–≤–∞–¥—Ü–∞—Ç—å",
    21: "–¥–≤–∞–¥—Ü–∞—Ç—å –æ–¥–∏–Ω", 22: "–¥–≤–∞–¥—Ü–∞—Ç—å –¥–≤–∞", 23: "–¥–≤–∞–¥—Ü–∞—Ç—å —Ç—Ä–∏"
}

# ===================== LOAD MODELS =====================
print("üîä –ó–∞–≥—Ä—É–∂–∞—é Vosk...")
if not os.path.exists(VOSK_MODEL_PATH):
    raise FileNotFoundError("–°–∫–∞—á–∞–π vosk-model-small-ru")

vosk_model = Model(VOSK_MODEL_PATH)

print("üó£Ô∏è –ó–∞–≥—Ä—É–∂–∞—é Silero TTS...")
tts_model, _ = torch.hub.load(
    'snakers4/silero-models',
    'silero_tts',
    language='ru',
    speaker='v4_ru'
)
tts_model.to(DEVICE)

# ===================== AUDIO =====================
def split_sentences(text):
    return re.split(r'(?<=[.!?]) +', text)

def speak(text: str):
    if not text.strip():
        return

    text = text.replace("–î–∂–∞—Ä–≤–∏—Å", "–î–∂+–∞—Ä–≤–∏—Å").replace("—Å—ç—Ä", "—Å+—ç—Ä")

    for k, v in WEATHER_REPLACE.items():
        text = text.replace(k, v)

    for sentence in split_sentences(text):
        audio = tts_model.apply_tts(
            text=sentence,
            speaker='eugene',
            sample_rate=48000,
            put_accent=True,
            put_yo=True
        )
        sd.play(audio, samplerate=48000)
        sd.wait()

def record_vad(
    max_seconds=6,
    silence_threshold=500,
    silence_duration=0.8
):
    print("üé§ –ì–æ–≤–æ—Ä–∏—Ç–µ...")
    chunk_duration = 0.1
    chunk_size = int(SAMPLE_RATE * chunk_duration)

    audio = []
    silence_chunks = 0
    max_chunks = int(max_seconds / chunk_duration)

    stream = sd.InputStream(
        samplerate=SAMPLE_RATE,
        channels=1,
        dtype='int16'
    )
    stream.start()

    for _ in range(max_chunks):
        chunk, _ = stream.read(chunk_size)
        audio.append(chunk)

        volume = np.abs(chunk).mean()
        silence_chunks = silence_chunks + 1 if volume < silence_threshold else 0

        if silence_chunks * chunk_duration > silence_duration:
            break

    stream.stop()
    stream.close()

    return np.concatenate(audio).tobytes()

def stt(audio_bytes):
    rec = KaldiRecognizer(vosk_model, SAMPLE_RATE)
    rec.AcceptWaveform(audio_bytes)
    return json.loads(rec.Result()).get("text", "").strip()

# ===================== INFO =====================
def get_weather():
    try:
        r = requests.get(f"https://wttr.in/{city}?format=%C+%t&lang=ru", timeout=6)
        return f"–ü–æ–≥–æ–¥–∞ –≤ {city}: {r.text.strip()}, —Å—ç—Ä."
    except:
        return "–ü–æ–≥–æ–¥–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, —Å—ç—Ä."

def get_time():
    h, m = datetime.now().hour, datetime.now().minute
    return f"–°–µ–π—á–∞—Å {HOURS[h]} {m} –º–∏–Ω—É—Ç, —Å—ç—Ä."

# ===================== GPT =====================
def gpt_query(text):
    global DIALOG_HISTORY

    DIALOG_HISTORY.append({"role": "user", "content": text})

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        *DIALOG_HISTORY[-MAX_HISTORY * 2:]
    ]

    resp = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        max_tokens=120,
        temperature=0.7,
        timeout=10
    )

    answer = resp.choices[0].message.content
    DIALOG_HISTORY.append({"role": "assistant", "content": answer})
    return answer

# ===================== MAIN =====================
if __name__ == "__main__":
    speak("–î–∂–∞—Ä–≤–∏—Å –æ–Ω–ª–∞–π–Ω. –ì–æ—Ç–æ–≤ –∫ –≤–∞—à–∏–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏—è–º, —Å—ç—Ä.")

    exit_cmds = ["–≤—ã—Ö–æ–¥", "—Å—Ç–æ–ø", "–ø–æ–∫–∞"]
    STOP = False

    while True:
        audio = record_vad()
        text = stt(audio)

        if not text:
            continue

        print("üë§:", text)
        lower = text.lower()

        if any(x in lower for x in exit_cmds):
            speak("–û—Ç–∫–ª—é—á–∞—é—Å—å. –í—Å–µ–≥–æ –¥–æ–±—Ä–æ–≥–æ, —Å—ç—Ä.")
            break

        if "–º—É–∑—ã–∫" in lower:
            webbrowser.open(random.choice(MUSIC_LINKS))
            speak("–ú—É–∑—ã–∫–∞ –∑–∞–ø—É—â–µ–Ω–∞, —Å—ç—Ä.")
            STOP = True
            continue

        if "–ø–æ–≥–æ–¥–∞" in lower:
            speak(get_weather())
            continue

        if "–≤—Ä–µ–º—è" in lower or "–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å" in lower:
            speak(get_time())
            continue

        answer = gpt_query(text)
        print("ü§ñ:", answer)
        speak(answer)
